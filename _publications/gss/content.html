<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- title and page icon -->
  <title>Gaussian Splatting in Style</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
  <!-- icons for buttons -->
  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="stylesheet" href="/assets/css/bulma.min.css">
  <link rel="stylesheet" href="/assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="/assets/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="/assets/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/slider-component.css">
  <link rel="stylesheet" href="/assets/css/index.css">
  <link rel="stylesheet" href="/assets/css/stylesheet.css">
  
  
  <link rel="icon" type="image/x-icon" href="assets/favicon.ico">
  
  <!-- <link href="/assets/css/fontawesome.all.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link href="/assets/css/index.css" rel="stylesheet"> -->

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script defer src="/js/fontawesome.all.min.js"></script>
  <script src="/js/index.js"></script> -->
</head>

<body>
  <center>
    <section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Gaussian Splatting in Style</h1>
                    <div class="is-size-4 publication-authors"><span class="author-block">CVPR, 2023</span></div>
                    <div class="is-size-5 publication-authors">
                        <div tyle="justify-content: space-between;">

                            <span class="author-block">
                                <a href="https://abhi-rf.github.io" target="_blank">Abhishek Saroha</a><sup>1,2</sup>
                            </span>

                            <span class="author-block">
                                <a href="https://cvg.cit.tum.de/members/gladkova" target="_blank">Mariia Gladkova</a><sup>1,2</sup>
                            </span>

                            <span class="author-block">
                                <a href="https://ceveloper.github.io/" target="_blank">Cecilia Curreli</a><sup>1,2</sup>
                            </span>                            
                            
                            
                            <span class="author-block">
                                <a href="https://dominikmuhle.github.io" target="_blank">Dominik  Muhle</a><sup>1,2</sup>
                            </span>

                            <span class="author-block">
                                <a href="https://cvg.cit.tum.de/members/yenamandra" target="_blank">Tarun Yenamandra</a><sup>1,2</sup>
                            </span>
                            
                            
                            <span class="author-block">
                                <a href="https://vision.in.tum.de/members/cremers" target="_blank">Daniel  Cremers</a><sup>1,2,3</sup>
                            </span>
                            
                        </div>
                    <div class="is-size-5 publication-affiliations">
                        
                        
                        
                        <span class="affiliation-block">
                            <sup>1</sup>TUM
                        </span>
                        
                        
                        
                        <span class="affiliation-block">
                            <sup>2</sup>Munich Center for Machine Learning
                        </span>
                        
                 
                        
                    </div>
                    <div class="column has-text-centered">
                        <div class="publication-links">
                            
                            
                            
                            
                            <span class="link-block">
                                <a href="https://arxiv.org/abs/2403.08498" target="_blank"
                                    class="external_link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        
                                        <i class="bi bi-file-earmark-richtext"></i>
                                        
                                    </span>
                                    <span>Paper</span>
                                </a>
                            </span>
                            
                            
                            
                            <!-- <span class="link-block">
                                <a href="https://github.com/DominikMuhle/dnls_covs" target="_blank"
                                    class="external_link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        
                                        <i class="bi bi-github"></i>
                                        
                                    </span>
                                    <span>Code</span>
                                </a>
                            </span>
                            
                            
                            
                            <span class="link-block">
                                <a href="https://www.youtube.com/watch?v=_wDUresP6v8&t=23s" target="_blank"
                                    class="external_link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        
                                        <i class="bi bi-youtube"></i>
                                        
                                    </span>
                                    <span>Video</span>
                                </a>
                            </span> -->
                            
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
  </center>
  <!-- Content -->
  <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video width="100%" autoplay="" muted="" loop="">
        <source src="./assets/qualitative.gif" type="video/mp4" />
      Your browser does not support the video tag.
      </video>
      <p class="content has-text-justified">
        <i><b> A comparison of our proposed method against some baselines.</i>
      </p>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D scene stylization extends the work of neural style transfer to 3D. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across multiple views. A vast majority of the previous works achieve this by training a 3D model for every stylized image and a set of multi-view images. In contrast, we propose a novel architecture trained on a collection of style images that, at test time, produces real time high-quality stylized novel views. We choose the underlying 3D scene representation for our model as 3D Gaussian splatting. We take the 3D Gaussians and process them using a multi-resolution hash grid and a tiny MLP to obtain stylized views. The MLP is conditioned on different style codes for generalization to different styles during test time. The explicit nature of 3D Gaussians gives us inherent advantages over NeRF-based methods, including geometric consistency and a fast training and rendering regime. This enables our method to be useful for various practical use cases, such as augmented or virtual reality. We demonstrate that our method achieves state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Overview</h2>
          <p class="content has-text-justified">
              <img width="100%" src="./assets/teaser.png" />
              <br /><br />
              a) We develop a novel state-of-the-art method, GSS, to perform neural scene stylization in real-time based on 3D Gaussian splatting. We are among the first to perform scene stylization using 3D Gaussians
              <br />
              b) We demonstrate the effectiveness of our method by comparing against various types of baselines both quantitatively and qualitatively across various real-world datasets across different settings.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Overview</h2>
          <p class="content has-text-justified">
              <img width="100%" src="./assets/pred_dense.png" />
              <br /><br />
              <i><b> Dense Uncertainty Prediction</b> Color-coded for visualization.</i>
              <br /><br />
              We want to estimate keypoint positional uncertainties $\boldsymbol{\Sigma}_{2\text{D}}, \boldsymbol{\Sigma}^\prime_{2\text{D}}$ in the images such that minimizing the PNEC energy function
              $$
                  \boldsymbol{R} = \text{arg min}_{\boldsymbol{R}} E(\boldsymbol{R}, \boldsymbol{t})
              $$
              $$
                  E(\boldsymbol{R}, \boldsymbol{t}) = \sum_i \frac{e_i^2}{\sigma_i^2} = \sum_i \frac{| \boldsymbol{t}^\top (\boldsymbol{f}_i \times \boldsymbol{R} \boldsymbol{f}^\prime_i) |^2}{\boldsymbol{t}^\top (\hat{(\boldsymbol{R} \boldsymbol{f}^\prime_i)} \boldsymbol{\Sigma}_i \hat{(\boldsymbol{R} \boldsymbol{f}^\prime_i)}{}^\top + \hat{\boldsymbol{f}_i} \boldsymbol{R} \boldsymbol{\Sigma}^\prime_i \boldsymbol{R}^\top \hat{\boldsymbol{f}_i}{}^\top) \boldsymbol{t}}
              $$
              leads to a minimal positional error (see paper for more details). Using implicit differentiation we can get the gradient of the pose error $e_{\text{rot}}$ with regard to the image uncertainties as
              $$
                  \frac{d \mathcal{L}}{d \boldsymbol{\Sigma}_{2\text{D}}} = - \frac{\partial^2 E_s}{\partial \boldsymbol{\Sigma}_{2\text{D}} \partial \boldsymbol{R}{}^\top} \left(\frac{\partial^2 E_s}{\partial \boldsymbol{R} \partial \boldsymbol{R}{}^\top} \right)^{-1} \frac{e_{\text{rot}}}{\partial \boldsymbol{R}}
              $$
              <br />
              allowing us to differentiate through to optimization and training an encoder-decoder network to predict dense uncertainty estimates for the whole image.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Architecture</h2>
          <p class="content has-text-justified">
              <img width="100%" src="./assets/architecture.png" />
              <br /><br />
              <i><b>Training Scheme Overview.</b></i>
              <br /><br />
              <h3 class="title is-4 has-text-left">Supervised Learning</h3>
              <p class="content has-text-justified">
                Given a dataset with accurate pose information we can train the encoder-decoder by comparing the estimated pose to the ground truth pose giving us following loss function:
                $$e_{\text{rot}} = \angle  \tilde{\boldsymbol{R}}{}^\top \boldsymbol{R}$$
              </p>
              <h3 class="title is-4 has-text-left">Self-Supervised Learning</h3>
              <p class="content has-text-justified">
                For datasets without accurate ground truth pose information our framework allows to train the encoder-decoder in a self-supervised manner by exploiting the cycle consistency between a tuple of images such that the pose error is given by:
                $$e_{\text{rot}}=\angle \prod_{(i,j) \in \mathcal{P}} \boldsymbol{R}_{ij}$$
              </p>
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Results</h2>
          <p class="content has-text-justified">
              We evaluate our framework using a combination of synthetic and real-world experiments. For the synthetic data, we investigate the ability of the gradient to learn the underlying noise distribution correctly by overfitting covariance estimates directly. We also investigate if better noise estimation leads to a reduces rotational error.
              <br />
              On real-world data, we use the gradient to train a network to predicts the noise distributions from images for different keypoint detectors. We train a network, both in a supervised and self-supervised manner, for SuperPoint and Basalt KLT-Tracks, since they follow different paradigms. We evaluate the performance of the learned covariances in a visual odometry setting on the popular KITTI odometry dataset. Results for the EuRoC dataset can be found in the paper.
              <h3 class="title is-4 has-text-left">Synthetic Experiments</h3>
              <img width="50%" src="./assets/target_covs.png" />
              <p class="content has-text-justified">
                <i><b>Synthetic Experiment.</b> Estimated covariances (red) compared to ground truth covariances (green).</i>
                <br /><br />
                In the simulated experiments we overfit covariance estimates for a single relative pose estimation problem. For this, we create random relative pose estimation problem consisting of two camera-frames observing points in 3D space. The points are projected into camera frames using a pinhole camera model. We fix the noise in the first frame to be small, isotropic, and homogeneous in nature. Each projected point in the second frame is assigned a random gaussian noise distribution. From this $128\,000$ random problems are sampled with random relative poses. We learn the noise distributions by initializing all covariance estimates as scaled identity matrices, solving the relative pose estimation problem using the PNEC and updating the parameters of the distribution using the gradient directly. The figure shows, that with implicit differentiation, our framework can learn the correct distributions from noisy data by following the gradient that minimizes the rotational error.
              </p>
              <h3 class="title is-4 has-text-left">Real World</h3>
              <img width="50%" src="./assets/trajectory_paper.png" />
              <p class="content has-text-justified">
                <i><b>Trajectory.</b> KITTI seq. 00. Visualization uses the ground truth translation scale as we do 2-view pose estimation.</i>
                <br /><br />
                We evaluate our method on the KITTI and EuRoC (see paper). For the supervised training of KITTI we choose sequences 00-07 as the training set and 08-10 as the test set. For the self-supervised training we also only train on sequences 00-07. We use a smaller UNet architecture as our network to predict the covariances for the whole image.
              </p>
              <img width="75%" src="./assets/table_superpoint.png" />
              <p class="content has-text-justified">
                <i><b>Visual Odometry Results.</b> Rotation and translation error using SuperPoint keypoints.</i>
              </p>
              <img width="75%" src="./assets/table_klt.png" />
              <p class="content has-text-justified">
                <i><b>Visual Odometry Results.</b> Rotation and translation error using KLT keypoints.</i>
                <br /><br />
                The tables show the average results on the test set over 5 runs for SuperPoint and KLT tracks on KITTI, respectively. We show additional results in the supplementary material. Our methods consistently perform the best over all sequences, with the self-supervised being on par with our supervised training. Compared to its non-probabilistic counterpart NEC-LS, our method improves $\text{RPE}_1$ by 7% and 13% and the $\text{RPE}_n$ by 37% and 23% for different keypoint detectors on unseen data. Furthermore, it also improves upon methods that use weighting, like weighted NEC-LS and the non-learned covariances for the PNEC, significantly.
              </p>
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->


  
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <!-- citation -->
            <h2>Citation <a onclick="CopyToClipboard('@article{saroha2024gaussian, title={Gaussian Splatting in Style}, author={Saroha, Abhishek and Gladkova, Mariia and Curreli, Cecilia and Yenamandra, Tarun and Cremers, Daniel}, journal={arXiv preprint arXiv:2403.08498}, year={2024} }')" class="btn btn--primary">
                    <i class="fa fa-copy"></i>
                </a></h2>
            <pre>
<code id="citation_block">
</code>
          </pre>
            <script src="/js/copy_to_clipboard.js"></script>
            <script>
                document.getElementById("citation_block").innerHTML = formateCitationHTML("@article{saroha2024gaussian, title={Gaussian Splatting in Style}, author={Saroha, Abhishek and Gladkova, Mariia and Curreli, Cecilia and Yenamandra, Tarun and Cremers, Daniel}, journal={arXiv preprint arXiv:2403.08498}, year={2024} }");
            </script>
        </div>
    </div>
</section>

  <!-- Acknowledgements -->

</body>

</html>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>